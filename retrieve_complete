import os
from pathlib import Path

# --- Put all caches under D: (fewer lock/permission issues than %LOCALAPPDATA%)
os.environ['PYSERINI_CACHE']   = r'D:\PyseriniCache'
os.environ['IR_DATASETS_HOME'] = r'D:\ir_datasets'

# Critical: move temp out of C:\Users\...\AppData\Local\Temp where AV/locks happen
os.environ['TMP']  = r'D:\ir_tmp'
os.environ['TEMP'] = r'D:\ir_tmp'

# (Optional) single-threaded downloads reduce chance of Windows file lock races
os.environ['IR_DATASETS_DL_THREADS'] = '1'

# Make sure folders exist
for p in [r'D:\PyseriniCache', r'D:\ir_datasets', r'D:\ir_tmp']:
    Path(p).mkdir(parents=True, exist_ok=True)

from pyserini.search.lucene import LuceneSearcher
import ir_datasets

# -------------------------
# Choose your TREC-DL setup
# -------------------------
TRECDL_YEAR = "2019"        # "2019" or "2020"
LEVEL       = "passage"     # "passage" or "document"

if LEVEL == "passage":
    DATASET_ID = f"msmarco-passage/trec-dl-{TRECDL_YEAR}/judged"
    INDEX_NAME = "msmarco-v1-passage"   # matches MS MARCO v1 passage IDs used by TREC-DL
elif LEVEL == "document":
    DATASET_ID = f"msmarco-document/trec-dl-{TRECDL_YEAR}/judged"
    INDEX_NAME = "msmarco-v1-doc"       # matches MS MARCO v1 document IDs used by TREC-DL
else:
    raise ValueError("LEVEL must be 'passage' or 'document'")

# Load TREC-DL dataset
ds = ir_datasets.load(DATASET_ID)

# Searcher for the matching collection
searcher = LuceneSearcher.from_prebuilt_index(INDEX_NAME)
# You can tweak BM25 if you like; these are MS MARCO-ish settings.
searcher.set_bm25(k1=0.82, b=0.68)

# Build qrels dict {qid: {docid: relevance}}
qrels = {}
for q in ds.qrels_iter():
    qrels.setdefault(str(q.query_id), {})[q.doc_id] = int(q.relevance)

# Helper to robustly extract the query text (field name differs across some datasets)
def _query_text(q):
    return getattr(q, "text", getattr(q, "title", getattr(q, "query", "")))

# TREC-DL has a small set of judged queries (e.g., 43 in 2019, 54 in 2020 for passage)
queries = list(ds.queries_iter())
query = queries[0]  # pick whichever you want
qid = str(query.query_id)
topic = _query_text(query)

print(f"Dataset: {DATASET_ID}")
print(f"Total queries loaded: {len(queries)}")
print(f"Query {qid}: {topic}\n")

# Retrieve
k = 10
hits = searcher.search(topic, k=k)

# Collect results
results = []
for i, h in enumerate(hits, 1):
    doc = searcher.doc(h.docid)
    doc_text = doc.raw() if doc is not None else ""
    results.append({
        "query": topic,
        "passage" if LEVEL == "passage" else "document": doc_text,
        "docid": h.docid,
        "score": h.score,
        "relevance": qrels.get(qid, {}).get(h.docid, None)  # may be None if unjudged
    })

# Write outputs
out_dir = Path("outputs"); out_dir.mkdir(exist_ok=True)

suffix = f"trecdl_{LEVEL}_{TRECDL_YEAR}_q{qid}"
labels_path = out_dir / f"labels_{suffix}.tsv"
with labels_path.open("w", encoding="utf-8", newline="") as f:
    f.write(f"# Dataset: {DATASET_ID}\n# Query ID: {qid}\n# Topic: {topic}\n")
    f.write("docid\trelevance\tscore\n")
    for r in results:
        f.write(f"{r['docid']}\t{r['relevance']}\t{r['score']:.4f}\n")

docs_path = out_dir / f"topic_and_docs_{suffix}.txt"
with docs_path.open("w", encoding="utf-8") as f:
    f.write(f"Dataset: {DATASET_ID}\nQuery ID: {qid}\nTopic: {topic}\n\n")
    for i, r in enumerate(results, 1):
        rel = r["relevance"]
        score = r["score"]
        f.write(f"Doc {i}: {r['docid']} (rel={rel}, score={score:.3f})\n")
        body_key = "passage" if LEVEL == "passage" else "document"
        f.write(("Passage:\n" if LEVEL == "passage" else "Document:\n") + (r[body_key] or "") + "\n" + "-"*80 + "\n\n")

print(f"Wrote: {labels_path}")
print(f"Wrote: {docs_path}")
